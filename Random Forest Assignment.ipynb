{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778665aa-9358-4939-b9fe-a83085101faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prepare a model for glass classification using Random Forest\n",
      "0                                  Data Description:          \n",
      "1                              RI : refractive index          \n",
      "2  Na: Sodium (unit measurement: weight percent i...          \n",
      "3                                      Mg: Magnesium          \n",
      "4                                       AI: Aluminum          \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming glass.xlsx is in the current working directory.\n",
    "# If it's in a different location, provide the full path.\n",
    "try:\n",
    "  df = pd.read_excel('glass.xlsx')\n",
    "  print(df.head()) # Display the first few rows to confirm successful load\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: 'glass.xlsx' not found. Please ensure the file exists in the correct location.\")\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58caa2f3-8dba-4b7c-b8b9-1be068e83b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prepare a model for glass classification using Random Forest\n",
      "0                                  Data Description:          \n",
      "1                              RI : refractive index          \n",
      "2  Na: Sodium (unit measurement: weight percent i...          \n",
      "3                                      Mg: Magnesium          \n",
      "4                                       AI: Aluminum          \n",
      "\n",
      "Missing Values:\n",
      "Prepare a model for glass classification using Random Forest    1\n",
      "dtype: int64\n",
      "\n",
      "Summary Statistics:\n",
      "       Prepare a model for glass classification using Random Forest\n",
      "count                                                  18          \n",
      "unique                                                 18          \n",
      "top                                     Data Description:          \n",
      "freq                                                    1          \n",
      "\n",
      "Data Types:\n",
      "Prepare a model for glass classification using Random Forest    object\n",
      "dtype: object\n",
      "\n",
      "Box Plots for Numerical Features:\n",
      "\n",
      "Unique Values in Categorical Columns:\n",
      "\n",
      "Column: Prepare a model for glass classification using Random Forest\n",
      "Prepare a model for glass classification using Random Forest\n",
      "Data Description:                                                                               1\n",
      "RI : refractive index                                                                           1\n",
      " 6 --tableware                                                                                  1\n",
      " 5 --containers                                                                                 1\n",
      " 4 --vehicle_windows_non_float_processed (none in this database)                                1\n",
      " 3 --vehicle_windows_float_processed                                                            1\n",
      " 2 --building_windows_non_float_processed                                                       1\n",
      "1 -- building_windows_float_processed                                                           1\n",
      "Type: Type of glass: (class attribute)                                                          1\n",
      "Fe: Iron                                                                                        1\n",
      "Ba: Barium                                                                                      1\n",
      "Ca: Calcium                                                                                     1\n",
      "K:Potassium                                                                                     1\n",
      "Si: Silicon                                                                                     1\n",
      "AI: Aluminum                                                                                    1\n",
      "Mg: Magnesium                                                                                   1\n",
      "Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)    1\n",
      " 7 --headlamps                                                                                  1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  1. Exploratory Data Analysis (EDA):\n",
    "# Perform exploratory data analysis to understand the structure of the dataset.\n",
    "# Check for missing values, outliers, inconsistencies in the data.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "  df = pd.read_excel('glass.xlsx')\n",
    "  print(df.head()) # Display the first few rows to confirm successful load\n",
    "\n",
    "  # Check for missing values\n",
    "  print(\"\\nMissing Values:\")\n",
    "  print(df.isnull().sum())\n",
    "\n",
    "  # Summary statistics\n",
    "  print(\"\\nSummary Statistics:\")\n",
    "  print(df.describe())\n",
    "\n",
    "  # Data types of each column\n",
    "  print(\"\\nData Types:\")\n",
    "  print(df.dtypes)\n",
    "\n",
    "  # Explore potential outliers using box plots\n",
    "  print(\"\\nBox Plots for Numerical Features:\")\n",
    "  numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "  for col in numerical_cols:\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      sns.boxplot(x=df[col])\n",
    "      plt.title(f'Box Plot of {col}')\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "  # Check for inconsistencies (e.g., unique values in categorical columns)\n",
    "  print(\"\\nUnique Values in Categorical Columns:\")\n",
    "  categorical_cols = df.select_dtypes(include=['object']).columns  # Identify object type columns\n",
    "  for col in categorical_cols:\n",
    "      print(f\"\\nColumn: {col}\")\n",
    "      print(df[col].value_counts())\n",
    "\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: 'glass.xlsx' not found. Please ensure the file exists in the correct location.\")\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f7ce2e8-cb51-4d75-bfa1-fd542ca53b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Histograms for Numerical Features:\n",
      "\n",
      "Pair Plot for Numerical Features:\n",
      "An error occurred: No variables found for grid columns.\n"
     ]
    }
   ],
   "source": [
    "#  2: Data Visualization:\n",
    "# Create visualizations such as histograms, box plots, or pair plots to visualize the distributions and relationships between features.\n",
    "# Analyze any patterns or correlations observed in the data.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel('glass.xlsx')\n",
    "\n",
    "    # Histograms\n",
    "    print(\"\\nHistograms for Numerical Features:\")\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(df[col], kde=True)  # Added KDE for better visualization\n",
    "        plt.title(f'Histogram of {col}')\n",
    "        plt.show()\n",
    "\n",
    "    # Pair plots\n",
    "    print(\"\\nPair Plot for Numerical Features:\")\n",
    "    sns.pairplot(df[numerical_cols], hue='Type', diag_kind='kde') #hue for better visualization\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Box plots (already present in your code)\n",
    "    print(\"\\nBox Plots for Numerical Features:\")\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Box Plot of {col}')\n",
    "        plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'glass.xlsx' not found. Please ensure the file exists in the correct location.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8c4a33d-c849-425e-8404-0a79c90d2ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Before Handling:\n",
      "Prepare a model for glass classification using Random Forest    1\n",
      "dtype: int64\n",
      "An error occurred: at least one array or dtype is required\n"
     ]
    }
   ],
   "source": [
    "#3: Data Preprocessing\n",
    "# 1. Check for missing values in the dataset and decide on a strategy for handling them.Implement the chosen strategy (e.g., imputation or removal) and explain your reasoning.\n",
    "# 2. If there are categorical variables, apply encoding techniques like one-hot encoding to convert them into numerical format.\n",
    "# 3. Apply feature scaling techniques such as standardization or normalization to ensure that all features are on a similar scale. Handling the imbalance data.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel('glass.xlsx')\n",
    "\n",
    "    # 1. Handling Missing Values (if any)\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing Values Before Handling:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Impute missing values with the mean for numerical features\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "    print(\"\\nMissing Values After Handling:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "\n",
    "    # 2. Encoding Categorical Features (if any)\n",
    "    # One-hot encode the 'Type' column (assuming it's categorical)\n",
    "    categorical_cols = ['Type']  # Specify your categorical columns here\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_cols)], remainder='passthrough')\n",
    "    df = pd.DataFrame(ct.fit_transform(df), columns=ct.get_feature_names_out())\n",
    "\n",
    "    # 3. Feature Scaling\n",
    "    # Scale numerical features using standardization\n",
    "    numerical_cols = [col for col in df.columns if col not in ct.get_feature_names_out(categorical_cols)]\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "\n",
    "    # 4. Handling Class Imbalance (if needed)\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(columns=[col for col in df.columns if 'Type' in col]) # Assuming 'Type' columns are generated by OneHotEncoder\n",
    "    y = df[[col for col in df.columns if 'Type' in col]]\n",
    "\n",
    "    # Apply SMOTE to oversample the minority classes\n",
    "    smote = SMOTE(random_state=42) # Use random_state for reproducibility\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "    # Combine resampled features and target into a new DataFrame\n",
    "    df_resampled = pd.concat([pd.DataFrame(X_resampled), y_resampled], axis=1)\n",
    "\n",
    "\n",
    "    print(\"\\nFirst few rows of the preprocessed DataFrame:\")\n",
    "    print(df_resampled.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'glass.xlsx' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9beac215-3eb3-4e40-918a-bdb0bf4a7355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n",
      "Precision: 0.9475975861278741\n",
      "Recall: 0.9473684210526315\n",
      "F1-score: 0.9474488711132564\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        64\n",
      "           1       0.96      0.95      0.96       107\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.94      0.95      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  4: Random Forest Model Implementation\n",
    "# 1. Divide the data into train and test split.\n",
    "# 2. Implement a Random Forest classifier using Python and a machine learning library like scikit-learn.\n",
    "# 3. Train the model on the train dataset. Evaluate the performance on test data using metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# 5. Random Forest Model Training and Evaluation\n",
    "rf_classifier = RandomForestClassifier(random_state=42) # You can tune hyperparameters here\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b82b9ff3-0e5f-4415-b629-a5ea0a0d8c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.9473684210526315\n",
      "Bagging Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93        64\n",
      "           1       0.98      0.93      0.96       107\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.94      0.95      0.94       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vogul\\anaconda4\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9532163742690059\n",
      "AdaBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        64\n",
      "           1       0.96      0.96      0.96       107\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.95      0.95      0.95       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n",
      "Gradient Boosting Accuracy: 0.9649122807017544\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        64\n",
      "           1       0.96      0.98      0.97       107\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.97      0.96      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n",
      "\n",
      "Comparison of Accuracy:\n",
      "Bagging: 0.9473684210526315\n",
      "AdaBoost: 0.9532163742690059\n",
      "Gradient Boosting: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load example dataset (replace with your own data if needed)\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Bagging\n",
    "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bagging_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_bagging = bagging_classifier.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "print(f\"Bagging Accuracy: {accuracy_bagging}\")\n",
    "print(f\"Bagging Classification Report:\\n{classification_report(y_test, y_pred_bagging)}\")\n",
    "\n",
    "# AdaBoost\n",
    "ada_boost_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_boost_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_adaboost = ada_boost_classifier.predict(X_test)\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_adaboost}\")\n",
    "print(f\"AdaBoost Classification Report:\\n{classification_report(y_test, y_pred_adaboost)}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_gb}\")\n",
    "print(f\"Gradient Boosting Classification Report:\\n{classification_report(y_test, y_pred_gb)}\")\n",
    "\n",
    "# Comparing the results\n",
    "print(\"\\nComparison of Accuracy:\")\n",
    "print(f\"Bagging: {accuracy_bagging}\")\n",
    "print(f\"AdaBoost: {accuracy_adaboost}\")\n",
    "print(f\"Gradient Boosting: {accuracy_gb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dd2ec83-607e-4a16-82bf-f1bf31828daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Notes:\n",
    "# 1. Explain Bagging and Boosting methods. How is it different from each other.\n",
    "# 2. Explain how to handle imbalance in the data.\n",
    "\n",
    "# Explanations of Bagging and Boosting, and handling class imbalance\n",
    "\n",
    "# 1. Bagging and Boosting\n",
    "\n",
    "# Bagging (Bootstrap Aggregating):\n",
    "# - Multiple base learners (e.g., decision trees) are trained on different subsets of the training data, created by random sampling with replacement (bootstrapping).\n",
    "# - Each base learner makes predictions, and the final prediction is an aggregate (usually the average or majority vote) of the individual predictions.\n",
    "# - Reduces variance by averaging out the errors of individual models.\n",
    "# - Example: Random Forest.\n",
    "\n",
    "# Boosting:\n",
    "# - Base learners are trained sequentially.\n",
    "# - Each subsequent learner focuses on the data points that the previous learners misclassified.\n",
    "# - Data points are weighted, with more weight given to misclassified points.\n",
    "# - Reduces bias by combining weak learners into a strong learner.\n",
    "# - Example: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "\n",
    "# Differences:\n",
    "# - Data sampling: Bagging uses random sampling with replacement, while boosting assigns weights to data points.\n",
    "# - Learner training: Bagging trains learners independently in parallel, while boosting trains them sequentially.\n",
    "# - Error handling: Bagging reduces variance by averaging out errors, while boosting reduces bias by focusing on misclassifications.\n",
    "\n",
    "\n",
    "# 2. Handling Class Imbalance\n",
    "\n",
    "# Class imbalance occurs when one class has significantly more examples than others in a dataset. This can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "# Methods to handle imbalance:\n",
    "\n",
    "# a. Oversampling: Increase the number of examples in the minority class.\n",
    "#    - SMOTE (Synthetic Minority Over-sampling Technique): Creates synthetic examples by interpolating between existing minority class examples.  Used in the provided code.\n",
    "#    - Random oversampling: Duplicates existing minority class examples.\n",
    "\n",
    "# b. Undersampling: Decrease the number of examples in the majority class.\n",
    "#    - Random undersampling: Randomly removes majority class examples.\n",
    "#    - NearMiss: Selects majority class examples based on their distance to minority class examples.\n",
    "\n",
    "# c. Hybrid methods: Combine oversampling and undersampling.\n",
    "\n",
    "# d. Cost-sensitive learning: Assign different misclassification costs to different classes.  The model is penalized more heavily for misclassifying the minority class.\n",
    "\n",
    "# e. Algorithm selection: Choose algorithms that are less sensitive to class imbalance (e.g., decision trees, random forests).\n",
    "\n",
    "\n",
    "# In the provided code:\n",
    "# - SMOTE is used for oversampling the minority classes before training the Random Forest Classifier.\n",
    "\n",
    "# Further considerations:\n",
    "\n",
    "# - Evaluate using appropriate metrics: Accuracy is not a good metric for imbalanced datasets.  Use precision, recall, F1-score, AUC-ROC, and AUC-PR instead.\n",
    "# - Experiment with different methods and hyperparameters:  Try different oversampling/undersampling techniques and different algorithms.\n",
    "# - Cross-validation: Use techniques like stratified k-fold cross-validation to get a better estimate of model performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402eade6-139c-46a9-910b-b39f24258bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
